
# ğŸ“‰ Telco Customer Churn Prediction

A production-style Machine Learning pipeline to predict customer churn using the Telco Customer Churn dataset.

The goal is to identify customers likely to leave so that businesses can take preventive action and reduce revenue loss.

---

## ğŸš€ Project Highlights

âœ… End-to-end ML pipeline (clean â†’ features â†’ model â†’ evaluation)  
âœ… Professional preprocessing using ColumnTransformer + Pipeline  
âœ… Feature engineering based on customer behavior  
âœ… Imbalance-aware training  
âœ… XGBoost & CatBoost models  
âœ… Threshold optimization for business recall  
âœ… Cross-validation  
âœ… Model explainability with SHAP  

---

## ğŸ“Š Problem Statement

Customer churn is a major issue in telecom services. Acquiring a new customer costs 5â€“7x more than retaining an existing one.

Objective:
Predict whether a customer will churn (Yes/No) using demographics, contracts, billing behavior, and service usage.

This is a binary classification problem with class imbalance (~27% churn).

---

## ğŸ§  Approach

Pipeline:

Data Cleaning  
â†’ EDA  
â†’ Feature Engineering  
â†’ Preprocessing Pipeline  
â†’ Baseline Model  
â†’ XGBoost/CatBoost  
â†’ Cross Validation  
â†’ Threshold Tuning  
â†’ Explainability (SHAP)

---

## ğŸ—‚ Dataset

Source: Kaggle â€“ Telco Customer Churn  

Rows: ~7,000 customers

Key features:
- tenure
- MonthlyCharges
- TotalCharges
- Contract
- InternetService
- PaymentMethod
- Services
- Churn (target)

---

## ğŸ§¹ Data Cleaning

- Converted TotalCharges to numeric
- Median imputation for missing values
- Dropped customerID
- Avoided chained assignment issues (pandas 2.x safe)

---

## ğŸ›  Feature Engineering

Added behavioral features:

- AvgCharge (spending pattern)
- IsNewCustomer (early churn risk)
- HasContract (loyalty signal)
- NumServices (engagement level)

These significantly improved model performance.

---

## âš™ï¸ Preprocessing

Implemented sklearn ColumnTransformer:

Numeric â†’ passthrough  
Categorical â†’ OneHotEncoder  

Benefits:
- No leakage
- Reproducible
- Cross-validation friendly
- Production ready

---

## ğŸ¤– Models Used

Baseline:
- Logistic Regression

Advanced:
- XGBoost
- CatBoost

Tree-based boosting performs best for tabular churn problems.

---

## ğŸ“ Evaluation Metrics

Because the dataset is imbalanced:

- ROC-AUC â­
- F1 Score â­
- Recall â­
- Confusion Matrix

---

## ğŸ“ˆ Results (Typical)

| Metric | Score |
|---------|---------|
| Accuracy | 80â€“85% |
| ROC-AUC | 0.88â€“0.90 |
| F1 Score | 0.70+ |
| Recall | 0.78+ |

---

## ğŸ” Explainability

SHAP used to interpret predictions:

- Month-to-month contracts â†’ high churn
- Fiber optic â†’ higher churn
- Short tenure â†’ higher churn

This provides actionable business insights.

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ customer_churn_prediction.ipynb  
â”œâ”€â”€ README.md  
â”œâ”€â”€ requirements.txt  
â””â”€â”€ data/

---

## â–¶ï¸ How to Run

pip install -r requirements.txt  
jupyter notebook customer_churn_prediction.ipynb

---

## ğŸ“¦ Requirements

pandas  
numpy  
scikit-learn  
xgboost  
catboost  
shap  
matplotlib  
seaborn  

---

## ğŸ¯ Key Learnings

- Feature engineering > model complexity
- Tree models dominate tabular data
- Threshold tuning improves business metrics
- Explainability matters in real applications

---

## ğŸ‘¤ Author

Machine Learning project demonstrating production-style tabular modeling.

---

â­ If you found this useful, please star the repo!
